% Experiment Section
\section{Methods}

\subsection{Data}

We utilized the AML scAtlas~\cite{amlAtlas2024}, a comprehensive resource containing scRNA-seq data from 159 AML patients across multiple studies. From this atlas, we selected five studies representing diverse technical platforms and biological contexts:

\begin{itemize}
    \item \textbf{van\_galen\_2019}~\cite{vangalen2019}: 23,344 cells from AML patients profiled using Seq-Well technology; serves as the gold-standard reference for AML cell type hierarchies
    \item \textbf{jiang\_2020}: 73,810 cells profiled using 10x Genomics Chromium
    \item \textbf{beneyto-calabuig-2023}: 75,372 cells using 10x Genomics Chromium Single Cell 3'
    \item \textbf{velten\_2021}: 4,191 cells using Muta-Seq (mutation tracking + transcriptomics)
    \item \textbf{zhang\_2023}: 75,962 cells from the atlas paper's primary dataset
\end{itemize}

All datasets included expert consensus annotations generated through the atlas pipeline: scVI integration, CellTypist + SingleR + scType consensus, manual marker gene curation, and custom LSC annotation. These annotations served as our reference labels for evaluation.

\subsection{Experiment 1: Annotation Replication}

\subsubsection{SCimilarity Projection}
We projected each dataset independently to SCimilarity's pre-trained latent space~\cite{scimilarity2023} (model version 1.1):
\begin{enumerate}
    \item Extracted raw count matrices from each study
    \item Aligned gene names to SCimilarity's gene order (embedding common genes, zero-padding missing genes)
    \item Computed embeddings in batches of 5,000 cells to manage memory
    \item Generated 384-dimensional latent representations for each cell
\end{enumerate}

\subsubsection{Clustering and Evaluation}
For each study, we performed Leiden clustering~\cite{leiden2019} on SCimilarity embeddings at multiple resolutions (0.1, 0.2, 0.3, 0.4, 0.5) to match the granularity of expert annotations (16 cell types). We computed:

\begin{itemize}
    \item \textbf{Adjusted Rand Index (ARI)}: Measures agreement between cluster assignments and expert labels, correcting for chance
    \item \textbf{Normalized Mutual Information (NMI)}: Quantifies shared information between clusterings
\end{itemize}

Resolution 0.1 was identified as optimal, producing approximately 14--17 clusters per study, closely matching the expert annotation granularity.

\subsection{Experiment 2: Intra-Study Label Transfer (Baseline)}

To establish a performance baseline without batch effects, we performed label transfer within a single study. Using van\_galen\_2019 (23,344 cells), we randomly split cells into training (80\%) and test (20\%) sets, ensuring no batch effects between reference and target. This provides an upper bound on achievable accuracy and isolates method-intrinsic performance from batch-related degradation.

Both traditional Random Forest and SCimilarity KNN were evaluated on this within-study split. We additionally performed 5-fold cross-validation for the traditional method to assess variance. Metrics (ARI, NMI, accuracy) were computed on the held-out test set.

\subsection{Experiment 3: Cross-Study Label Transfer}

We evaluated cross-study label transfer performance using van\_galen\_2019 as reference (23,344 labeled cells) and the remaining four studies as targets. This inter-study setting introduces batch effects from different sequencing platforms, processing protocols, and patient cohorts.

\subsubsection{Traditional Reference-Based Classification}
Following standard approaches analogous to SingleR~\cite{singleR2019} and Seurat~\cite{seurat2021}:
\begin{enumerate}
    \item Identified common genes between reference and target (typically >15,000 genes)
    \item Normalized both datasets (library size normalization, log transformation)
    \item Selected 2,000 highly variable genes from reference
    \item Trained Random Forest classifier (100 trees, max depth 20) on reference
    \item Predicted cell type labels for target dataset
\end{enumerate}

\subsubsection{SCimilarity KNN Transfer}
Leveraging the pre-trained latent space:
\begin{enumerate}
    \item Projected reference cells to SCimilarity space
    \item Projected target cells to SCimilarity space
    \item Performed k-nearest neighbors classification (k=15) in shared latent space
    \item Transferred labels from reference neighbors to target cells
\end{enumerate}

\subsubsection{Evaluation Metrics}
We assessed label transfer quality using:
\begin{itemize}
    \item \textbf{ARI}: Overall agreement with expert target annotations
    \item \textbf{NMI}: Information preservation during transfer
    \item \textbf{Macro F1}: Average F1 score across all cell types (equal weight)
    \item \textbf{Weighted F1}: F1 score weighted by cell type frequency
    \item \textbf{Transfer time}: Computational cost per target dataset
\end{itemize}

Macro F1 is particularly important as it reveals performance on rare cell types (e.g., leukemic stem cells, progenitors) that are often critical for AML biology but constitute a small fraction of cells.

\subsection{Experiment 4: Batch Correction Benchmarking}

To directly quantify SCimilarity's batch correction capabilities, we performed standardized batch correction benchmarking using scib~\cite{scib2021} (single-cell integration benchmarking) metrics. We compared four methods on a pairwise integration task (van\_galen\_2019 vs setty\_2019, N=53,898 cells):

\begin{itemize}
    \item \textbf{Uncorrected}: Raw normalized counts (baseline)
    \item \textbf{scVI}~\cite{scvi2018}: Variational autoencoder explicitly trained for batch correction
    \item \textbf{SCimilarity}: Pre-trained foundation model embeddings (no explicit batch correction training)
    \item \textbf{Harmony}~\cite{harmony2019}: PCA-based batch correction
\end{itemize}

The scib framework evaluates integration quality across two complementary dimensions:

\textbf{Batch correction metrics} (higher = better mixing):
\begin{itemize}
    \item Silhouette batch: Separation of batches in latent space (lower = better mixing)
    \item iLISI: Integration local inverse Simpson's index (batch mixing in local neighborhoods)
    \item KBET: k-nearest neighbor batch effect test
    \item Graph connectivity: Preservation of cell-cell connections across batches
\end{itemize}

\textbf{Biological conservation metrics} (higher = better preservation):
\begin{itemize}
    \item Silhouette label: Separation of cell types in latent space
    \item cLISI: Cell-type local inverse Simpson's index (cell type purity)
    \item PCR comparison: Principal component regression agreement with uncorrected data
\end{itemize}

Each method produces aggregate scores for Batch correction, Bio conservation, and Total (balanced combination). This provides an orthogonal evaluation of batch robustness complementary to the downstream label transfer performance measured in Experiment 3.

\subsection{Computational Environment}

All analyses were performed using Python 3.9 with scanpy~\cite{scanpy2018} (v1.9.3), SCimilarity (v1.1), scikit-learn (v1.3.0), and standard scientific computing libraries. Computations were run on [specify your hardware if relevant].

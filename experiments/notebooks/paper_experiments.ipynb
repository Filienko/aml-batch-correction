{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Experiments: SCimilarity for AML Annotation\n",
    "\n",
    "This notebook runs all experiments to support the paper claims.\n",
    "\n",
    "**Note**: Update `DATA_PATH` below to point to your AML_scAtlas.h5ad file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add SCCL to path\n",
    "sys.path.insert(0, '/home/user/aml-batch-correction')\n",
    "\n",
    "# Core imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# SCCL imports\n",
    "from sccl import Pipeline\n",
    "from sccl.data import subset_data, preprocess_data\n",
    "from sccl.evaluation import (\n",
    "    compute_metrics, \n",
    "    compute_per_class_metrics,\n",
    "    compute_confusion_stats,\n",
    "    plot_confusion_matrix,\n",
    "    plot_umap\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "DATA_PATH = \"/home/daniilf/full_aml_tasks/batch_correction/data/AML_scAtlas.h5ad\"\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úì Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "print(\"Loading AML scAtlas...\")\n",
    "adata = sc.read_h5ad(DATA_PATH)\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"  Cells: {adata.n_obs:,}\")\n",
    "print(f\"  Genes: {adata.n_vars:,}\")\n",
    "print(f\"  Studies: {adata.obs['study'].nunique()}\")\n",
    "print(f\"  Cell Types: {adata.obs['cell_type'].nunique()}\")\n",
    "\n",
    "# Show cell type distribution\n",
    "print(\"\\nüî¨ Cell Type Distribution:\")\n",
    "print(adata.obs['cell_type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Annotation Replication\n",
    "\n",
    "**Question**: Can SCimilarity approximate the expert consensus pipeline?\n",
    "\n",
    "**Expected**: ARI > 0.70 indicates good replication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to Van Galen studies\n",
    "van_galen_studies = [\n",
    "    'van_galen_2019',\n",
    "    'zhang_2023',\n",
    "    'beneyto-calabuig-2023',\n",
    "    'jiang_2020',\n",
    "    'velten_2021',\n",
    "    'zhai_2022',\n",
    "]\n",
    "\n",
    "available_studies = adata.obs['study'].unique()\n",
    "valid_studies = [s for s in van_galen_studies if s in available_studies]\n",
    "\n",
    "print(f\"Using {len(valid_studies)} Van Galen studies:\")\n",
    "for study in valid_studies:\n",
    "    n_cells = (adata.obs['study'] == study).sum()\n",
    "    print(f\"  ‚Ä¢ {study}: {n_cells:,} cells\")\n",
    "\n",
    "adata_vg = subset_data(adata, studies=valid_studies)\n",
    "print(f\"\\nSubset: {adata_vg.n_obs:,} cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SCimilarity\n",
    "print(\"Running SCimilarity predictions...\")\n",
    "pipeline = Pipeline(model=\"scimilarity\", batch_key=\"study\")\n",
    "predictions = pipeline.predict(adata_vg.copy(), target_column=\"cell_type\")\n",
    "\n",
    "# Compute metrics\n",
    "print(\"\\nComputing metrics...\")\n",
    "metrics = compute_metrics(\n",
    "    y_true=adata_vg.obs['cell_type'].values,\n",
    "    y_pred=predictions,\n",
    "    adata=adata_vg,\n",
    "    metrics=['accuracy', 'ari', 'nmi', 'f1']\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*60)\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"  {metric.upper():20s}: {value:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Interpretation\n",
    "ari = metrics['ari']\n",
    "if ari > 0.80:\n",
    "    print(\"\\n‚úÖ EXCELLENT: SCimilarity closely approximates expert consensus\")\n",
    "elif ari > 0.70:\n",
    "    print(\"\\n‚úÖ GOOD: SCimilarity approximates expert consensus well\")\n",
    "elif ari > 0.60:\n",
    "    print(\"\\n‚ö†Ô∏è MODERATE: Some agreement but room for improvement\")\n",
    "else:\n",
    "    print(\"\\n‚ùå LOW: Significant discrepancy from expert annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class performance\n",
    "per_class = compute_per_class_metrics(\n",
    "    y_true=adata_vg.obs['cell_type'].values,\n",
    "    y_pred=predictions\n",
    ")\n",
    "\n",
    "per_class_df = pd.DataFrame(per_class).T.sort_values('support')\n",
    "\n",
    "print(\"Per-Class Performance (sorted by rarity):\")\n",
    "print(per_class_df)\n",
    "\n",
    "# Identify rare types\n",
    "threshold = adata_vg.n_obs * 0.01\n",
    "rare_types = per_class_df[per_class_df['support'] < threshold]\n",
    "\n",
    "if len(rare_types) > 0:\n",
    "    print(f\"\\nüî¨ Rare Cell Types (< 1% frequency):\")\n",
    "    print(rare_types[['f1', 'precision', 'recall', 'support']])\n",
    "    print(f\"\\n  Average F1 on rare types: {rare_types['f1'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "fig = plot_confusion_matrix(\n",
    "    y_true=adata_vg.obs['cell_type'].values,\n",
    "    y_pred=predictions,\n",
    "    normalize=True,\n",
    "    figsize=(14, 12)\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Label Transfer Benchmark\n",
    "\n",
    "**Question**: Is SCimilarity better than traditional ML for cross-study transfer?\n",
    "\n",
    "**Setup**: Train on van_galen_2019, test on other studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Transfer Benchmark\n",
    "reference_study = 'van_galen_2019'\n",
    "query_studies = [s for s in valid_studies if s != reference_study][:2]  # Test first 2 for speed\n",
    "\n",
    "print(f\"Reference: {reference_study}\")\n",
    "print(f\"Query studies: {query_studies}\")\n",
    "\n",
    "# Prepare reference\n",
    "adata_ref = subset_data(adata_vg, studies=[reference_study])\n",
    "\n",
    "# Models to test\n",
    "models_to_test = {\n",
    "    'SCimilarity': 'scimilarity',\n",
    "    'Random Forest': 'random_forest',\n",
    "    'SVM': 'svm',\n",
    "    'KNN': 'knn',\n",
    "}\n",
    "\n",
    "# Results storage\n",
    "transfer_results = []\n",
    "\n",
    "# Test each query study\n",
    "for query_study in query_studies:\n",
    "    print(f\"\\nTesting on: {query_study}\")\n",
    "    adata_query = subset_data(adata_vg, studies=[query_study])\n",
    "    \n",
    "    for model_name, model_type in models_to_test.items():\n",
    "        print(f\"  {model_name}...\", end=' ')\n",
    "        \n",
    "        try:\n",
    "            pipeline = Pipeline(model=model_type)\n",
    "            \n",
    "            # Train if needed\n",
    "            if hasattr(pipeline.model, 'fit'):\n",
    "                adata_ref_prep = preprocess_data(adata_ref.copy(), batch_key=None)\n",
    "                pipeline.model.fit(adata_ref_prep, target_column='cell_type')\n",
    "            \n",
    "            # Predict\n",
    "            adata_query_prep = preprocess_data(adata_query.copy(), batch_key=None)\n",
    "            pred = pipeline.model.predict(adata_query_prep, target_column=None)\n",
    "            \n",
    "            # Evaluate\n",
    "            metrics = compute_metrics(\n",
    "                y_true=adata_query.obs['cell_type'].values,\n",
    "                y_pred=pred,\n",
    "                metrics=['accuracy', 'ari', 'nmi', 'f1_macro']\n",
    "            )\n",
    "            \n",
    "            transfer_results.append({\n",
    "                'model': model_name,\n",
    "                'query_study': query_study,\n",
    "                'accuracy': metrics['accuracy'],\n",
    "                'ari': metrics['ari'],\n",
    "                'nmi': metrics['nmi'],\n",
    "                'f1': metrics['f1_macro']\n",
    "            })\n",
    "            \n",
    "            print(f\"‚úì ARI: {metrics['ari']:.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error: {e}\")\n",
    "\n",
    "# Show results\n",
    "transfer_df = pd.DataFrame(transfer_results)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(transfer_df)\n",
    "\n",
    "# Average by model\n",
    "print(\"\\nAverage by Model:\")\n",
    "avg_by_model = transfer_df.groupby('model')[['accuracy', 'ari', 'f1']].mean()\n",
    "print(avg_by_model.sort_values('ari', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Computational Efficiency\n",
    "\n",
    "**Question**: How fast is SCimilarity compared to traditional pipeline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Subsample for timing\n",
    "adata_timing = subset_data(adata_vg.copy(), n_cells=min(5000, adata_vg.n_obs))\n",
    "print(f\"Timing on {adata_timing.n_obs:,} cells\")\n",
    "\n",
    "timing_results = []\n",
    "\n",
    "# Time SCimilarity\n",
    "print(\"\\nTiming SCimilarity...\")\n",
    "start = time.time()\n",
    "pipeline_scim = Pipeline(model=\"scimilarity\")\n",
    "pred_scim = pipeline_scim.predict(adata_timing.copy())\n",
    "scim_time = time.time() - start\n",
    "timing_results.append({'method': 'SCimilarity', 'time_seconds': scim_time})\n",
    "print(f\"  ‚úì {scim_time:.1f} seconds ({scim_time/60:.2f} minutes)\")\n",
    "\n",
    "# Time Random Forest\n",
    "print(\"\\nTiming Random Forest...\")\n",
    "start = time.time()\n",
    "pipeline_rf = Pipeline(model=\"random_forest\")\n",
    "pred_rf = pipeline_rf.predict(adata_timing.copy(), target_column='cell_type')\n",
    "rf_time = time.time() - start\n",
    "timing_results.append({'method': 'Random Forest', 'time_seconds': rf_time})\n",
    "print(f\"  ‚úì {rf_time:.1f} seconds ({rf_time/60:.2f} minutes)\")\n",
    "\n",
    "# Estimate traditional pipeline\n",
    "traditional_time = 27 * 60  # CellTypist + SingleR + scType (27 minutes)\n",
    "timing_results.append({'method': 'Traditional Pipeline', 'time_seconds': traditional_time})\n",
    "\n",
    "# Show results\n",
    "timing_df = pd.DataFrame(timing_results)\n",
    "timing_df['time_minutes'] = timing_df['time_seconds'] / 60\n",
    "timing_df['speedup'] = traditional_time / timing_df['time_seconds']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TIMING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(timing_df[['method', 'time_minutes', 'speedup']])\n",
    "\n",
    "print(f\"\\nSpeedup: {traditional_time/scim_time:.1f}x faster than traditional pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Report\n",
    "\n",
    "All experiments completed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PAPER EXPERIMENTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Annotation Replication\")\n",
    "print(f\"   ARI: {metrics['ari']:.4f}\")\n",
    "print(f\"   Accuracy: {metrics['accuracy']:.4f}\")\n",
    "\n",
    "if 'transfer_df' in locals():\n",
    "    print(\"\\n2. Label Transfer\")\n",
    "    avg = transfer_df.groupby('model')['ari'].mean()\n",
    "    best = avg.idxmax()\n",
    "    print(f\"   Best: {best} (ARI: {avg[best]:.3f})\")\n",
    "\n",
    "if 'timing_df' in locals():\n",
    "    print(\"\\n3. Computational Efficiency\")\n",
    "    print(f\"   SCimilarity: {scim_time/60:.1f} min\")\n",
    "    print(f\"   Speedup: {traditional_time/scim_time:.1f}x\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ All experiments completed!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
